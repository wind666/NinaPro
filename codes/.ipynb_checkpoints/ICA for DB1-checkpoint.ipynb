{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "np.random.seed(0)\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "from sklearn.decomposition import FastICA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read EMG signals from 15 subjects (DB1,Exercise B) 10 channes and 17+1 classes\n",
    "EMG=[]\n",
    "subject=[]\n",
    "lab=[]\n",
    "for j in range(1,16):\n",
    "    dire=(\"C:/Users/HP/Desktop/git_clone/NinaPro/unzip_data/\"+'S' + str(j) + '_' + 'A1_E2.mat')\n",
    "    ninapro_DB1=loadmat(dire)\n",
    "    \n",
    "    lab.extend(ninapro_DB1['restimulus'])\n",
    "\n",
    "    sub=(ninapro_DB1['subject'])\n",
    "    su=[sub[0][0]]*len(ninapro_DB1['restimulus'])\n",
    "    subject.extend(su)\n",
    "    \n",
    "    EMG.extend(ninapro_DB1['emg'])\n",
    "#2-D array to 2-D list\n",
    "#sub = [[row.flat[0] for row in line] for line in subject]\n",
    "label = [[row.flat[0] for row in line] for line in lab]\n",
    "data = [[row.flat[0] for row in line] for line in EMG]\n",
    "#change label format to (n_samples,) . Also data format is (n_samples,n_features)\n",
    "label=(np.ravel(label)).tolist()\n",
    "np.shape(label),np.shape(data),np.shape(subject)\n",
    "# down sampling to 1 KHz\n",
    "subject=subject[::2]\n",
    "label=label[::2]\n",
    "data=data[::2]\n",
    "# Define a dictionary containing EMG data \n",
    "dataemg = {'subject': subject,\n",
    "           'label':label,\n",
    "           'data': data}\n",
    "#dataemg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove LAbel 0\n",
    "\"\"\"found all index in list\"\"\"\n",
    "def all_indices(value, qlist):\n",
    "    indices = []\n",
    "    idx = -1\n",
    "    while True:\n",
    "        try:\n",
    "            idx = qlist.index(value, idx+1)\n",
    "            indices.append(idx)\n",
    "        except ValueError:\n",
    "            break\n",
    "    return indices\n",
    "class0=all_indices(0,dataemg['label'])\n",
    "#class1=all_indices(1,dataemg['label'])\n",
    "#class2=all_indices(2,dataemg['label'])\n",
    "\n",
    "#make a copy from dataset\n",
    "import copy\n",
    "\n",
    "#emg_17class=dataemg.copy()\n",
    "emg_17class = copy.deepcopy(dataemg)\n",
    "\n",
    "#delete class 0 from dataset\n",
    "for index in sorted(class0, reverse=True):\n",
    "    del emg_17class['subject'][index]\n",
    "    del emg_17class['label'][index]\n",
    "    del emg_17class['data'][index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA(max_iter=1000)\n",
    "ISA_17class_ = ica.fit_transform(emg_17class['data'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors\n",
      "accuracy = 0.80\n",
      "accuracy of train data=0.91\n",
      "Decision Tree\n",
      "accuracy = 0.65\n",
      "accuracy of train data=0.98\n",
      "Random Forest\n",
      "accuracy = 0.73\n",
      "accuracy of train data=0.99\n",
      "AdaBoost\n",
      "accuracy = 0.19\n",
      "accuracy of train data=0.19\n",
      "Naive Bayes\n",
      "accuracy = 0.18\n",
      "accuracy of train data=0.18\n",
      "LDA\n",
      "accuracy = 0.19\n",
      "accuracy of train data=0.19\n",
      "QDA\n",
      "accuracy = 0.23\n",
      "accuracy of train data=0.23\n",
      "Iteration 1, loss = 2.82886669\n",
      "Iteration 2, loss = 2.80982481\n",
      "Iteration 3, loss = 2.76048767\n",
      "Iteration 4, loss = 2.71128536\n",
      "Iteration 5, loss = 2.67842201\n",
      "Iteration 6, loss = 2.65650044\n",
      "Iteration 7, loss = 2.64190496\n",
      "Iteration 8, loss = 2.63151079\n",
      "Iteration 9, loss = 2.62388029\n",
      "Iteration 10, loss = 2.61740192\n",
      "Iteration 11, loss = 2.61230688\n",
      "Iteration 12, loss = 2.60758164\n",
      "Iteration 13, loss = 2.60371470\n",
      "Iteration 14, loss = 2.59979525\n",
      "Iteration 15, loss = 2.59650389\n",
      "Iteration 16, loss = 2.59320903\n",
      "Iteration 17, loss = 2.59020785\n",
      "Iteration 18, loss = 2.58745887\n",
      "Iteration 19, loss = 2.58494487\n",
      "Iteration 20, loss = 2.58251408\n",
      "Iteration 21, loss = 2.58038131\n",
      "Iteration 22, loss = 2.57831195\n",
      "Iteration 23, loss = 2.57674664\n",
      "Iteration 24, loss = 2.57497561\n",
      "Iteration 25, loss = 2.57322780\n",
      "Iteration 26, loss = 2.57184149\n",
      "Iteration 27, loss = 2.57061616\n",
      "Iteration 28, loss = 2.56935119\n",
      "Iteration 29, loss = 2.56818064\n",
      "Iteration 30, loss = 2.56697210\n",
      "Iteration 31, loss = 2.56572088\n",
      "Iteration 32, loss = 2.56484032\n",
      "Iteration 33, loss = 2.56390301\n",
      "Iteration 34, loss = 2.56282903\n",
      "Iteration 35, loss = 2.56181366\n",
      "Iteration 36, loss = 2.56075431\n",
      "Iteration 37, loss = 2.55992368\n",
      "Iteration 38, loss = 2.55882267\n",
      "Iteration 39, loss = 2.55798998\n",
      "Iteration 40, loss = 2.55715720\n",
      "Iteration 41, loss = 2.55637264\n",
      "Iteration 42, loss = 2.55533471\n",
      "Iteration 43, loss = 2.55446657\n",
      "Iteration 44, loss = 2.55371684\n",
      "Iteration 45, loss = 2.55292345\n",
      "Iteration 46, loss = 2.55193223\n",
      "Iteration 47, loss = 2.55137278\n",
      "Iteration 48, loss = 2.55030320\n",
      "Iteration 49, loss = 2.54954670\n",
      "Iteration 50, loss = 2.54884852\n",
      "Iteration 51, loss = 2.54792171\n",
      "Iteration 52, loss = 2.54709699\n",
      "Iteration 53, loss = 2.54642911\n",
      "Iteration 54, loss = 2.54569339\n",
      "Iteration 55, loss = 2.54502858\n",
      "Iteration 56, loss = 2.54425720\n",
      "Iteration 57, loss = 2.54347251\n",
      "Iteration 58, loss = 2.54299942\n",
      "Iteration 59, loss = 2.54214521\n",
      "Iteration 60, loss = 2.54158757\n",
      "Iteration 61, loss = 2.54092004\n",
      "Iteration 62, loss = 2.54041875\n",
      "Iteration 63, loss = 2.53980919\n",
      "Iteration 64, loss = 2.53922702\n",
      "Iteration 65, loss = 2.53864734\n",
      "Iteration 66, loss = 2.53814228\n",
      "Iteration 67, loss = 2.53730135\n",
      "Iteration 68, loss = 2.53681637\n",
      "Iteration 69, loss = 2.53624128\n",
      "Iteration 70, loss = 2.53559423\n",
      "Iteration 71, loss = 2.53501284\n",
      "Iteration 72, loss = 2.53446882\n",
      "Iteration 73, loss = 2.53383549\n",
      "Iteration 74, loss = 2.53314902\n",
      "Iteration 75, loss = 2.53274405\n",
      "Iteration 76, loss = 2.53219702\n",
      "Iteration 77, loss = 2.53146152\n",
      "Iteration 78, loss = 2.53086351\n",
      "Iteration 79, loss = 2.53018614\n",
      "Iteration 80, loss = 2.52973132\n",
      "Iteration 81, loss = 2.52925483\n",
      "Iteration 82, loss = 2.52854051\n",
      "Iteration 83, loss = 2.52786387\n",
      "Iteration 84, loss = 2.52747133\n",
      "Iteration 85, loss = 2.52693943\n",
      "Iteration 86, loss = 2.52658452\n",
      "Iteration 87, loss = 2.52584721\n",
      "Iteration 88, loss = 2.52534745\n",
      "Iteration 89, loss = 2.52498308\n",
      "Iteration 90, loss = 2.52448351\n",
      "Iteration 91, loss = 2.52382330\n",
      "Iteration 92, loss = 2.52358509\n",
      "Iteration 93, loss = 2.52309085\n",
      "Iteration 94, loss = 2.52280191\n",
      "Iteration 95, loss = 2.52244329\n",
      "Iteration 96, loss = 2.52204764\n",
      "Iteration 97, loss = 2.52161447\n",
      "Iteration 98, loss = 2.52118548\n",
      "Iteration 99, loss = 2.52093899\n",
      "Iteration 100, loss = 2.52071408\n",
      "Iteration 101, loss = 2.51994769\n",
      "Iteration 102, loss = 2.51991072\n",
      "Iteration 103, loss = 2.51941072\n",
      "Iteration 104, loss = 2.51920747\n",
      "Iteration 105, loss = 2.51862724\n",
      "Iteration 106, loss = 2.51848628\n",
      "Iteration 107, loss = 2.51816926\n",
      "Iteration 108, loss = 2.51764020\n",
      "Iteration 109, loss = 2.51742728\n",
      "Iteration 110, loss = 2.51733382\n",
      "Iteration 111, loss = 2.51656066\n",
      "Iteration 112, loss = 2.51622977\n",
      "Iteration 113, loss = 2.51603193\n",
      "Iteration 114, loss = 2.51578125\n",
      "Iteration 115, loss = 2.51551862\n",
      "Iteration 116, loss = 2.51520405\n",
      "Iteration 117, loss = 2.51483195\n",
      "Iteration 118, loss = 2.51458923\n",
      "Iteration 119, loss = 2.51411698\n",
      "Iteration 120, loss = 2.51403281\n",
      "Iteration 121, loss = 2.51353614\n",
      "Iteration 122, loss = 2.51334566\n",
      "Iteration 123, loss = 2.51312953\n",
      "Iteration 124, loss = 2.51283603\n",
      "Iteration 125, loss = 2.51222529\n",
      "Iteration 126, loss = 2.51214180\n",
      "Iteration 127, loss = 2.51206019\n",
      "Iteration 128, loss = 2.51150791\n",
      "Iteration 129, loss = 2.51163761\n",
      "Iteration 130, loss = 2.51127695\n",
      "Iteration 131, loss = 2.51102968\n",
      "Iteration 132, loss = 2.51082229\n",
      "Iteration 133, loss = 2.51061062\n",
      "Iteration 134, loss = 2.51067023\n",
      "Iteration 135, loss = 2.51047602\n",
      "Iteration 136, loss = 2.51019013\n",
      "Iteration 137, loss = 2.51001587\n",
      "Iteration 138, loss = 2.50958673\n",
      "Iteration 139, loss = 2.50955464\n",
      "Iteration 140, loss = 2.50938287\n",
      "Iteration 141, loss = 2.50910766\n",
      "Iteration 142, loss = 2.50935259\n",
      "Iteration 143, loss = 2.50892635\n",
      "Iteration 144, loss = 2.50890082\n",
      "Iteration 145, loss = 2.50853383\n",
      "Iteration 146, loss = 2.50891666\n",
      "Iteration 147, loss = 2.50839865\n",
      "Iteration 148, loss = 2.50834294\n",
      "Iteration 149, loss = 2.50849072\n",
      "Iteration 150, loss = 2.50805696\n",
      "Iteration 151, loss = 2.50814795\n",
      "Iteration 152, loss = 2.50764627\n",
      "Iteration 153, loss = 2.50767154\n",
      "Iteration 154, loss = 2.50769996\n",
      "Iteration 155, loss = 2.50739951\n",
      "Iteration 156, loss = 2.50742561\n",
      "Iteration 157, loss = 2.50749694\n",
      "Iteration 158, loss = 2.50718814\n",
      "Iteration 159, loss = 2.50724028\n",
      "Iteration 160, loss = 2.50717139\n",
      "Iteration 161, loss = 2.50686059\n",
      "Iteration 162, loss = 2.50692968\n",
      "Iteration 163, loss = 2.50680561\n",
      "Iteration 164, loss = 2.50669245\n",
      "Iteration 165, loss = 2.50649507\n",
      "Iteration 166, loss = 2.50656646\n",
      "Iteration 167, loss = 2.50630171\n",
      "Iteration 168, loss = 2.50631113\n",
      "Iteration 169, loss = 2.50608868\n",
      "Iteration 170, loss = 2.50605360\n",
      "Iteration 171, loss = 2.50608814\n",
      "Iteration 172, loss = 2.50578112\n",
      "Iteration 173, loss = 2.50574903\n",
      "Iteration 174, loss = 2.50569616\n",
      "Iteration 175, loss = 2.50578526\n",
      "Iteration 176, loss = 2.50548227\n",
      "Iteration 177, loss = 2.50539017\n",
      "Iteration 178, loss = 2.50527713\n",
      "Iteration 179, loss = 2.50551531\n",
      "Iteration 180, loss = 2.50522393\n",
      "Iteration 181, loss = 2.50522357\n",
      "Iteration 182, loss = 2.50504686\n",
      "Iteration 183, loss = 2.50489953\n",
      "Iteration 184, loss = 2.50493905\n",
      "Iteration 185, loss = 2.50486476\n",
      "Iteration 186, loss = 2.50470942\n",
      "Iteration 187, loss = 2.50452894\n",
      "Iteration 188, loss = 2.50451984\n",
      "Iteration 189, loss = 2.50426503\n",
      "Iteration 190, loss = 2.50434029\n",
      "Iteration 191, loss = 2.50422724\n",
      "Iteration 192, loss = 2.50405832\n",
      "Iteration 193, loss = 2.50423139\n",
      "Iteration 194, loss = 2.50396962\n",
      "Iteration 195, loss = 2.50378015\n",
      "Iteration 196, loss = 2.50394751\n",
      "Iteration 197, loss = 2.50395878\n",
      "Iteration 198, loss = 2.50362515\n",
      "Iteration 199, loss = 2.50361498\n",
      "Iteration 200, loss = 2.50351368\n",
      "Iteration 201, loss = 2.50343766\n",
      "Iteration 202, loss = 2.50317598\n",
      "Iteration 203, loss = 2.50321997\n",
      "Iteration 204, loss = 2.50307731\n",
      "Iteration 205, loss = 2.50295668\n",
      "Iteration 206, loss = 2.50302444\n",
      "Iteration 207, loss = 2.50268565\n",
      "Iteration 208, loss = 2.50250495\n",
      "Iteration 209, loss = 2.50260106\n",
      "Iteration 210, loss = 2.50255953\n",
      "Iteration 211, loss = 2.50227212\n",
      "Iteration 212, loss = 2.50224226\n",
      "Iteration 213, loss = 2.50212438\n",
      "Iteration 214, loss = 2.50216352\n",
      "Iteration 215, loss = 2.50182953\n",
      "Iteration 216, loss = 2.50162809\n",
      "Iteration 217, loss = 2.50162873\n",
      "Iteration 218, loss = 2.50148368\n",
      "Iteration 219, loss = 2.50145089\n",
      "Iteration 220, loss = 2.50125716\n",
      "Iteration 221, loss = 2.50120078\n",
      "Iteration 222, loss = 2.50098281\n",
      "Iteration 223, loss = 2.50096283\n",
      "Iteration 224, loss = 2.50064617\n",
      "Iteration 225, loss = 2.50059062\n",
      "Iteration 226, loss = 2.50052819\n",
      "Iteration 227, loss = 2.50040760\n",
      "Iteration 228, loss = 2.50039639\n",
      "Iteration 229, loss = 2.50001178\n",
      "Iteration 230, loss = 2.49976226\n",
      "Iteration 231, loss = 2.49998761\n",
      "Iteration 232, loss = 2.49956571\n",
      "Iteration 233, loss = 2.49955409\n",
      "Iteration 234, loss = 2.49942695\n",
      "Iteration 235, loss = 2.49917068\n",
      "Iteration 236, loss = 2.49887077\n",
      "Iteration 237, loss = 2.49904596\n",
      "Iteration 238, loss = 2.49882525\n",
      "Iteration 239, loss = 2.49886469\n",
      "Iteration 240, loss = 2.49876063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 241, loss = 2.49851210\n",
      "Iteration 242, loss = 2.49853286\n",
      "Iteration 243, loss = 2.49834489\n",
      "Iteration 244, loss = 2.49820026\n",
      "Iteration 245, loss = 2.49811547\n",
      "Iteration 246, loss = 2.49796042\n",
      "Iteration 247, loss = 2.49804023\n",
      "Iteration 248, loss = 2.49791800\n",
      "Iteration 249, loss = 2.49789505\n",
      "Iteration 250, loss = 2.49779244\n",
      "Iteration 251, loss = 2.49761756\n",
      "Iteration 252, loss = 2.49744917\n",
      "Iteration 253, loss = 2.49746243\n",
      "Iteration 254, loss = 2.49734760\n",
      "Iteration 255, loss = 2.49725324\n",
      "Iteration 256, loss = 2.49730717\n",
      "Iteration 257, loss = 2.49713407\n",
      "Iteration 258, loss = 2.49696765\n",
      "Iteration 259, loss = 2.49714707\n",
      "Iteration 260, loss = 2.49690742\n",
      "Iteration 261, loss = 2.49676642\n",
      "Iteration 262, loss = 2.49672786\n",
      "Iteration 263, loss = 2.49667325\n",
      "Iteration 264, loss = 2.49636667\n",
      "Iteration 265, loss = 2.49669995\n",
      "Iteration 266, loss = 2.49643513\n",
      "Iteration 267, loss = 2.49638775\n",
      "Iteration 268, loss = 2.49640677\n",
      "Iteration 269, loss = 2.49634022\n",
      "Iteration 270, loss = 2.49607865\n",
      "Iteration 271, loss = 2.49597725\n",
      "Iteration 272, loss = 2.49610658\n",
      "Iteration 273, loss = 2.49573380\n",
      "Iteration 274, loss = 2.49569118\n",
      "Iteration 275, loss = 2.49561209\n",
      "Iteration 276, loss = 2.49568717\n",
      "Iteration 277, loss = 2.49550877\n",
      "Iteration 278, loss = 2.49546103\n",
      "Iteration 279, loss = 2.49547110\n",
      "Iteration 280, loss = 2.49531380\n",
      "Iteration 281, loss = 2.49531064\n",
      "Iteration 282, loss = 2.49504629\n",
      "Iteration 283, loss = 2.49494774\n",
      "Iteration 284, loss = 2.49488593\n",
      "Iteration 285, loss = 2.49490342\n",
      "Iteration 286, loss = 2.49502912\n",
      "Iteration 287, loss = 2.49514658\n",
      "Iteration 288, loss = 2.49490180\n",
      "Iteration 289, loss = 2.49477922\n",
      "Iteration 290, loss = 2.49502572\n",
      "Iteration 291, loss = 2.49474665\n",
      "Iteration 292, loss = 2.49461116\n",
      "Iteration 293, loss = 2.49455346\n",
      "Iteration 294, loss = 2.49476403\n",
      "Iteration 295, loss = 2.49461617\n",
      "Iteration 296, loss = 2.49476173\n",
      "Iteration 297, loss = 2.49437588\n",
      "Iteration 298, loss = 2.49462559\n",
      "Iteration 299, loss = 2.49456706\n",
      "Iteration 300, loss = 2.49447411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "accuracy = 0.27\n",
      "accuracy of train data=0.27\n"
     ]
    }
   ],
   "source": [
    "# All classifiers\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "names = [\"Nearest Neighbors\",  \"Decision Tree\",\n",
    "         \"Random Forest\", \"AdaBoost\", \"Naive Bayes\", \"LDA\", \"QDA\",\"MLP\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(n_neighbors=3),\n",
    "    DecisionTreeClassifier(max_depth=30),\n",
    "    RandomForestClassifier(max_depth=30, n_estimators=10, max_features=1),#clf = RF(max_depth=200, random_state=0)\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    LDA(),\n",
    "    QDA(),\n",
    "    MLPClassifier(hidden_layer_sizes=(100,),alpha=0.002,verbose=True ,  max_iter=300)]\n",
    "\n",
    "#X = StandardScaler().fit_transform(emg_17class['data'])\n",
    "\n",
    "x_train , x_test,y_train,y_test=train_test_split(ISA_17class_ , emg_17class['label'],test_size=0.25,random_state=42,stratify=emg_17class['label']) \n",
    "\n",
    "\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "# iterate over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred=clf.predict(x_test)\n",
    "    print(name)\n",
    "    print(\"accuracy = %.2f\"%accuracy_score(y_test,y_pred))\n",
    "    y_pred_train=clf.predict(x_train)\n",
    "    accuracy=np.mean(y_pred_train==y_train)\n",
    "    print(\"accuracy of train data=%.2f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors\n",
      "accuracy = 0.80\n",
      "accuracy of train data=0.91\n",
      "Decision Tree\n",
      "accuracy = 0.67\n",
      "accuracy of train data=1.00\n",
      "Random Forest\n",
      "accuracy = 0.75\n",
      "accuracy of train data=1.00\n",
      "AdaBoost\n",
      "accuracy = 0.20\n",
      "accuracy of train data=0.20\n",
      "Naive Bayes\n",
      "accuracy = 0.18\n",
      "accuracy of train data=0.18\n",
      "LDA\n",
      "accuracy = 0.19\n",
      "accuracy of train data=0.19\n",
      "QDA\n",
      "accuracy = 0.27\n",
      "accuracy of train data=0.27\n",
      "Iteration 1, loss = 2.83116044\n",
      "Iteration 2, loss = 2.82325751\n",
      "Iteration 3, loss = 2.77845579\n",
      "Iteration 4, loss = 2.70432706\n",
      "Iteration 5, loss = 2.65562510\n",
      "Iteration 6, loss = 2.62906350\n",
      "Iteration 7, loss = 2.60771561\n",
      "Iteration 8, loss = 2.59107740\n",
      "Iteration 9, loss = 2.57942006\n",
      "Iteration 10, loss = 2.57058104\n",
      "Iteration 11, loss = 2.56318617\n",
      "Iteration 12, loss = 2.55609500\n",
      "Iteration 13, loss = 2.54811662\n",
      "Iteration 14, loss = 2.53941058\n",
      "Iteration 15, loss = 2.52974620\n",
      "Iteration 16, loss = 2.51890165\n",
      "Iteration 17, loss = 2.50773513\n",
      "Iteration 18, loss = 2.49775907\n",
      "Iteration 19, loss = 2.48907614\n",
      "Iteration 20, loss = 2.48241766\n",
      "Iteration 21, loss = 2.47601109\n",
      "Iteration 22, loss = 2.47031704\n",
      "Iteration 23, loss = 2.46568603\n",
      "Iteration 24, loss = 2.46101170\n",
      "Iteration 25, loss = 2.45707002\n",
      "Iteration 26, loss = 2.45250642\n",
      "Iteration 27, loss = 2.44849552\n",
      "Iteration 28, loss = 2.44416406\n",
      "Iteration 29, loss = 2.43980663\n",
      "Iteration 30, loss = 2.43486103\n",
      "Iteration 31, loss = 2.42988614\n",
      "Iteration 32, loss = 2.42522815\n",
      "Iteration 33, loss = 2.42067214\n",
      "Iteration 34, loss = 2.41675130\n",
      "Iteration 35, loss = 2.41226658\n",
      "Iteration 36, loss = 2.40817365\n",
      "Iteration 37, loss = 2.40470907\n",
      "Iteration 38, loss = 2.40135356\n",
      "Iteration 39, loss = 2.39809357\n",
      "Iteration 40, loss = 2.39516298\n",
      "Iteration 41, loss = 2.39224640\n",
      "Iteration 42, loss = 2.38943300\n",
      "Iteration 43, loss = 2.38658084\n",
      "Iteration 44, loss = 2.38395511\n",
      "Iteration 45, loss = 2.38095295\n",
      "Iteration 46, loss = 2.37771318\n",
      "Iteration 47, loss = 2.37542457\n",
      "Iteration 48, loss = 2.37280328\n",
      "Iteration 49, loss = 2.36934225\n",
      "Iteration 50, loss = 2.36658562\n",
      "Iteration 51, loss = 2.36347408\n",
      "Iteration 52, loss = 2.36119901\n",
      "Iteration 53, loss = 2.35811778\n",
      "Iteration 54, loss = 2.35477184\n",
      "Iteration 55, loss = 2.35155793\n",
      "Iteration 56, loss = 2.34859720\n",
      "Iteration 57, loss = 2.34535308\n",
      "Iteration 58, loss = 2.34212338\n",
      "Iteration 59, loss = 2.33885010\n",
      "Iteration 60, loss = 2.33516044\n",
      "Iteration 61, loss = 2.33281974\n",
      "Iteration 62, loss = 2.32952983\n",
      "Iteration 63, loss = 2.32671528\n",
      "Iteration 64, loss = 2.32443153\n",
      "Iteration 65, loss = 2.32202143\n",
      "Iteration 66, loss = 2.31986095\n",
      "Iteration 67, loss = 2.31753879\n",
      "Iteration 68, loss = 2.31546929\n",
      "Iteration 69, loss = 2.31381541\n",
      "Iteration 70, loss = 2.31179995\n",
      "Iteration 71, loss = 2.30954076\n",
      "Iteration 72, loss = 2.30834545\n",
      "Iteration 73, loss = 2.30664629\n",
      "Iteration 74, loss = 2.30493565\n",
      "Iteration 75, loss = 2.30380546\n",
      "Iteration 76, loss = 2.30218322\n",
      "Iteration 77, loss = 2.30088677\n",
      "Iteration 78, loss = 2.29931992\n",
      "Iteration 79, loss = 2.29855568\n",
      "Iteration 80, loss = 2.29738781\n",
      "Iteration 81, loss = 2.29599503\n",
      "Iteration 82, loss = 2.29462277\n",
      "Iteration 83, loss = 2.29367396\n",
      "Iteration 84, loss = 2.29235216\n",
      "Iteration 85, loss = 2.29196555\n",
      "Iteration 86, loss = 2.29031395\n",
      "Iteration 87, loss = 2.28974267\n",
      "Iteration 88, loss = 2.28848967\n",
      "Iteration 89, loss = 2.28738506\n",
      "Iteration 90, loss = 2.28646540\n",
      "Iteration 91, loss = 2.28545061\n",
      "Iteration 92, loss = 2.28464790\n",
      "Iteration 93, loss = 2.28393122\n",
      "Iteration 94, loss = 2.28288210\n",
      "Iteration 95, loss = 2.28222363\n",
      "Iteration 96, loss = 2.28091255\n",
      "Iteration 97, loss = 2.27985493\n",
      "Iteration 98, loss = 2.27941768\n",
      "Iteration 99, loss = 2.27878909\n",
      "Iteration 100, loss = 2.27789456\n",
      "Iteration 101, loss = 2.27654594\n",
      "Iteration 102, loss = 2.27594278\n",
      "Iteration 103, loss = 2.27516406\n",
      "Iteration 104, loss = 2.27434979\n",
      "Iteration 105, loss = 2.27367260\n",
      "Iteration 106, loss = 2.27307149\n",
      "Iteration 107, loss = 2.27147668\n",
      "Iteration 108, loss = 2.27127232\n",
      "Iteration 109, loss = 2.27000337\n",
      "Iteration 110, loss = 2.26984895\n",
      "Iteration 111, loss = 2.26843189\n",
      "Iteration 112, loss = 2.26798640\n",
      "Iteration 113, loss = 2.26746750\n",
      "Iteration 114, loss = 2.26674853\n",
      "Iteration 115, loss = 2.26574241\n",
      "Iteration 116, loss = 2.26479482\n",
      "Iteration 117, loss = 2.26375361\n",
      "Iteration 118, loss = 2.26336781\n",
      "Iteration 119, loss = 2.26243977\n",
      "Iteration 120, loss = 2.26147586\n",
      "Iteration 121, loss = 2.26093185\n",
      "Iteration 122, loss = 2.26030352\n",
      "Iteration 123, loss = 2.26005429\n",
      "Iteration 124, loss = 2.25894460\n",
      "Iteration 125, loss = 2.25798715\n",
      "Iteration 126, loss = 2.25694237\n",
      "Iteration 127, loss = 2.25673565\n",
      "Iteration 128, loss = 2.25554040\n",
      "Iteration 129, loss = 2.25536409\n",
      "Iteration 130, loss = 2.25474687\n",
      "Iteration 131, loss = 2.25335909\n",
      "Iteration 132, loss = 2.25258695\n",
      "Iteration 133, loss = 2.25219891\n",
      "Iteration 134, loss = 2.25148680\n",
      "Iteration 135, loss = 2.25046102\n",
      "Iteration 136, loss = 2.25050427\n",
      "Iteration 137, loss = 2.24880724\n",
      "Iteration 138, loss = 2.24855419\n",
      "Iteration 139, loss = 2.24818620\n",
      "Iteration 140, loss = 2.24703971\n",
      "Iteration 141, loss = 2.24624032\n",
      "Iteration 142, loss = 2.24529244\n",
      "Iteration 143, loss = 2.24530324\n",
      "Iteration 144, loss = 2.24432349\n",
      "Iteration 145, loss = 2.24426108\n",
      "Iteration 146, loss = 2.24307800\n",
      "Iteration 147, loss = 2.24260938\n",
      "Iteration 148, loss = 2.24125233\n",
      "Iteration 149, loss = 2.24103342\n",
      "Iteration 150, loss = 2.24055143\n",
      "Iteration 151, loss = 2.23939050\n",
      "Iteration 152, loss = 2.23892639\n",
      "Iteration 153, loss = 2.23814000\n",
      "Iteration 154, loss = 2.23775715\n",
      "Iteration 155, loss = 2.23727191\n",
      "Iteration 156, loss = 2.23647176\n",
      "Iteration 157, loss = 2.23572005\n",
      "Iteration 158, loss = 2.23518645\n",
      "Iteration 159, loss = 2.23375973\n",
      "Iteration 160, loss = 2.23344984\n",
      "Iteration 161, loss = 2.23325929\n",
      "Iteration 162, loss = 2.23230412\n",
      "Iteration 163, loss = 2.23128309\n",
      "Iteration 164, loss = 2.23102592\n",
      "Iteration 165, loss = 2.23019462\n",
      "Iteration 166, loss = 2.22974285\n",
      "Iteration 167, loss = 2.22904408\n",
      "Iteration 168, loss = 2.22886997\n",
      "Iteration 169, loss = 2.22808889\n",
      "Iteration 170, loss = 2.22698323\n",
      "Iteration 171, loss = 2.22627164\n",
      "Iteration 172, loss = 2.22587758\n",
      "Iteration 173, loss = 2.22584497\n",
      "Iteration 174, loss = 2.22446061\n",
      "Iteration 175, loss = 2.22388212\n",
      "Iteration 176, loss = 2.22365972\n",
      "Iteration 177, loss = 2.22280153\n",
      "Iteration 178, loss = 2.22212474\n",
      "Iteration 179, loss = 2.22157570\n",
      "Iteration 180, loss = 2.22090738\n",
      "Iteration 181, loss = 2.22049724\n",
      "Iteration 182, loss = 2.21884296\n",
      "Iteration 183, loss = 2.21906008\n",
      "Iteration 184, loss = 2.21793357\n",
      "Iteration 185, loss = 2.21713357\n",
      "Iteration 186, loss = 2.21647880\n",
      "Iteration 187, loss = 2.21572396\n",
      "Iteration 188, loss = 2.21525033\n",
      "Iteration 189, loss = 2.21426133\n",
      "Iteration 190, loss = 2.21358159\n",
      "Iteration 191, loss = 2.21313692\n",
      "Iteration 192, loss = 2.21189611\n",
      "Iteration 193, loss = 2.21197482\n",
      "Iteration 194, loss = 2.21086506\n",
      "Iteration 195, loss = 2.20969916\n",
      "Iteration 196, loss = 2.20950775\n",
      "Iteration 197, loss = 2.20823636\n",
      "Iteration 198, loss = 2.20769167\n",
      "Iteration 199, loss = 2.20687989\n",
      "Iteration 200, loss = 2.20612743\n",
      "Iteration 201, loss = 2.20533165\n",
      "Iteration 202, loss = 2.20538519\n",
      "Iteration 203, loss = 2.20421255\n",
      "Iteration 204, loss = 2.20372909\n",
      "Iteration 205, loss = 2.20286538\n",
      "Iteration 206, loss = 2.20237143\n",
      "Iteration 207, loss = 2.20163935\n",
      "Iteration 208, loss = 2.20191099\n",
      "Iteration 209, loss = 2.20063549\n",
      "Iteration 210, loss = 2.19998030\n",
      "Iteration 211, loss = 2.19969910\n",
      "Iteration 212, loss = 2.19953507\n",
      "Iteration 213, loss = 2.19877195\n",
      "Iteration 214, loss = 2.19784094\n",
      "Iteration 215, loss = 2.19735170\n",
      "Iteration 216, loss = 2.19764757\n",
      "Iteration 217, loss = 2.19625526\n",
      "Iteration 218, loss = 2.19630157\n",
      "Iteration 219, loss = 2.19545448\n",
      "Iteration 220, loss = 2.19515284\n",
      "Iteration 221, loss = 2.19494670\n",
      "Iteration 222, loss = 2.19442816\n",
      "Iteration 223, loss = 2.19404361\n",
      "Iteration 224, loss = 2.19309591\n",
      "Iteration 225, loss = 2.19282083\n",
      "Iteration 226, loss = 2.19174536\n",
      "Iteration 227, loss = 2.19157636\n",
      "Iteration 228, loss = 2.19130233\n",
      "Iteration 229, loss = 2.19066551\n",
      "Iteration 230, loss = 2.19030501\n",
      "Iteration 231, loss = 2.19045778\n",
      "Iteration 232, loss = 2.18920982\n",
      "Iteration 233, loss = 2.18936348\n",
      "Iteration 234, loss = 2.18888042\n",
      "Iteration 235, loss = 2.18817628\n",
      "Iteration 236, loss = 2.18775394\n",
      "Iteration 237, loss = 2.18734600\n",
      "Iteration 238, loss = 2.18762079\n",
      "Iteration 239, loss = 2.18671235\n",
      "Iteration 240, loss = 2.18657475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 241, loss = 2.18583229\n",
      "Iteration 242, loss = 2.18565516\n",
      "Iteration 243, loss = 2.18491019\n",
      "Iteration 244, loss = 2.18536071\n",
      "Iteration 245, loss = 2.18441348\n",
      "Iteration 246, loss = 2.18437121\n",
      "Iteration 247, loss = 2.18393880\n",
      "Iteration 248, loss = 2.18283219\n",
      "Iteration 249, loss = 2.18277899\n",
      "Iteration 250, loss = 2.18268725\n",
      "Iteration 251, loss = 2.18259997\n",
      "Iteration 252, loss = 2.18200819\n",
      "Iteration 253, loss = 2.18186648\n",
      "Iteration 254, loss = 2.18113046\n",
      "Iteration 255, loss = 2.18006139\n",
      "Iteration 256, loss = 2.18014175\n",
      "Iteration 257, loss = 2.18032147\n",
      "Iteration 258, loss = 2.17915773\n",
      "Iteration 259, loss = 2.17968964\n",
      "Iteration 260, loss = 2.17914226\n",
      "Iteration 261, loss = 2.17900342\n",
      "Iteration 262, loss = 2.17799847\n",
      "Iteration 263, loss = 2.17900487\n",
      "Iteration 264, loss = 2.17794499\n",
      "Iteration 265, loss = 2.17738088\n",
      "Iteration 266, loss = 2.17708193\n",
      "Iteration 267, loss = 2.17723090\n",
      "Iteration 268, loss = 2.17720275\n",
      "Iteration 269, loss = 2.17591784\n",
      "Iteration 270, loss = 2.17615358\n",
      "Iteration 271, loss = 2.17566273\n",
      "Iteration 272, loss = 2.17530733\n",
      "Iteration 273, loss = 2.17543876\n",
      "Iteration 274, loss = 2.17538300\n",
      "Iteration 275, loss = 2.17487769\n",
      "Iteration 276, loss = 2.17384202\n",
      "Iteration 277, loss = 2.17429554\n",
      "Iteration 278, loss = 2.17381605\n",
      "Iteration 279, loss = 2.17439836\n",
      "Iteration 280, loss = 2.17326759\n",
      "Iteration 281, loss = 2.17352660\n",
      "Iteration 282, loss = 2.17298735\n",
      "Iteration 283, loss = 2.17229302\n",
      "Iteration 284, loss = 2.17220440\n",
      "Iteration 285, loss = 2.17185021\n",
      "Iteration 286, loss = 2.17256597\n",
      "Iteration 287, loss = 2.17139615\n",
      "Iteration 288, loss = 2.17157360\n",
      "Iteration 289, loss = 2.17144629\n",
      "Iteration 290, loss = 2.17080300\n",
      "Iteration 291, loss = 2.17100442\n",
      "Iteration 292, loss = 2.17050683\n",
      "Iteration 293, loss = 2.17007191\n",
      "Iteration 294, loss = 2.17037810\n",
      "Iteration 295, loss = 2.16993895\n",
      "Iteration 296, loss = 2.16891825\n",
      "Iteration 297, loss = 2.16971711\n",
      "Iteration 298, loss = 2.16860749\n",
      "Iteration 299, loss = 2.16900129\n",
      "Iteration 300, loss = 2.16887493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "accuracy = 0.32\n",
      "accuracy of train data=0.32\n"
     ]
    }
   ],
   "source": [
    "# All classifiers\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "names = [\"Nearest Neighbors\",  \"Decision Tree\",\n",
    "         \"Random Forest\", \"AdaBoost\", \"Naive Bayes\", \"LDA\", \"QDA\",\"MLP\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(n_neighbors=3),\n",
    "    DecisionTreeClassifier(max_depth=30),\n",
    "    RandomForestClassifier(max_depth=30, n_estimators=10, max_features=1),#clf = RF(max_depth=200, random_state=0)\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    LDA(),\n",
    "    QDA(),\n",
    "    MLPClassifier(hidden_layer_sizes=(25,20),alpha=0.002,verbose=True ,  max_iter=300)]\n",
    "\n",
    "#X = StandardScaler().fit_transform(emg_17class['data'])\n",
    "\n",
    "x_train , x_test,y_train,y_test=train_test_split(ISA_17class_ , emg_17class['label'],test_size=0.25,random_state=42,stratify=emg_17class['label']) \n",
    "\n",
    "\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "# iterate over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred=clf.predict(x_test)\n",
    "    print(name)\n",
    "    print(\"accuracy = %.2f\"%accuracy_score(y_test,y_pred))\n",
    "    y_pred_train=clf.predict(x_train)\n",
    "    accuracy=np.mean(y_pred_train==y_train)\n",
    "    print(\"accuracy of train data=%.2f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors\n",
      "accuracy = 0.78\n",
      "accuracy of train data=0.89\n",
      "Decision Tree\n",
      "accuracy = 0.69\n",
      "accuracy of train data=1.00\n",
      "Random Forest\n",
      "accuracy = 0.75\n",
      "accuracy of train data=1.00\n",
      "AdaBoost\n",
      "accuracy = 0.20\n",
      "accuracy of train data=0.20\n",
      "Naive Bayes\n",
      "accuracy = 0.17\n",
      "accuracy of train data=0.17\n",
      "LDA\n",
      "accuracy = 0.19\n",
      "accuracy of train data=0.19\n",
      "QDA\n",
      "accuracy = 0.27\n",
      "accuracy of train data=0.27\n",
      "Iteration 1, loss = 2.49167372\n",
      "Iteration 2, loss = 2.24048703\n",
      "Iteration 3, loss = 2.15010003\n",
      "Iteration 4, loss = 2.09505959\n",
      "Iteration 5, loss = 2.05542578\n",
      "Iteration 6, loss = 2.02363832\n",
      "Iteration 7, loss = 1.99901435\n",
      "Iteration 8, loss = 1.97940351\n",
      "Iteration 9, loss = 1.96320167\n",
      "Iteration 10, loss = 1.94969989\n",
      "Iteration 11, loss = 1.93749610\n",
      "Iteration 12, loss = 1.92610703\n",
      "Iteration 13, loss = 1.91636664\n",
      "Iteration 14, loss = 1.90711108\n",
      "Iteration 15, loss = 1.89931204\n",
      "Iteration 16, loss = 1.89230641\n",
      "Iteration 17, loss = 1.88592253\n",
      "Iteration 18, loss = 1.88012973\n",
      "Iteration 19, loss = 1.87436111\n",
      "Iteration 20, loss = 1.86940089\n",
      "Iteration 21, loss = 1.86414526\n",
      "Iteration 22, loss = 1.85998948\n",
      "Iteration 23, loss = 1.85567970\n",
      "Iteration 24, loss = 1.85174829\n",
      "Iteration 25, loss = 1.84806890\n",
      "Iteration 26, loss = 1.84452656\n",
      "Iteration 27, loss = 1.84094663\n",
      "Iteration 28, loss = 1.83745209\n",
      "Iteration 29, loss = 1.83414009\n",
      "Iteration 30, loss = 1.83144912\n",
      "Iteration 31, loss = 1.82831342\n",
      "Iteration 32, loss = 1.82541440\n",
      "Iteration 33, loss = 1.82280567\n",
      "Iteration 34, loss = 1.81970881\n",
      "Iteration 35, loss = 1.81674637\n",
      "Iteration 36, loss = 1.81465101\n",
      "Iteration 37, loss = 1.81190738\n",
      "Iteration 38, loss = 1.80913467\n",
      "Iteration 39, loss = 1.80646149\n",
      "Iteration 40, loss = 1.80457903\n",
      "Iteration 41, loss = 1.80161183\n",
      "Iteration 42, loss = 1.79929968\n",
      "Iteration 43, loss = 1.79669005\n",
      "Iteration 44, loss = 1.79471662\n",
      "Iteration 45, loss = 1.79237680\n",
      "Iteration 46, loss = 1.78992904\n",
      "Iteration 47, loss = 1.78785467\n",
      "Iteration 48, loss = 1.78572926\n",
      "Iteration 49, loss = 1.78338201\n",
      "Iteration 50, loss = 1.78190543\n",
      "Iteration 51, loss = 1.77993000\n",
      "Iteration 52, loss = 1.77795566\n",
      "Iteration 53, loss = 1.77607019\n",
      "Iteration 54, loss = 1.77436871\n",
      "Iteration 55, loss = 1.77243261\n",
      "Iteration 56, loss = 1.77048187\n",
      "Iteration 57, loss = 1.76940689\n",
      "Iteration 58, loss = 1.76755097\n",
      "Iteration 59, loss = 1.76558883\n",
      "Iteration 60, loss = 1.76432205\n",
      "Iteration 61, loss = 1.76248126\n",
      "Iteration 62, loss = 1.76158336\n",
      "Iteration 63, loss = 1.75925696\n",
      "Iteration 64, loss = 1.75755130\n",
      "Iteration 65, loss = 1.75607279\n",
      "Iteration 66, loss = 1.75481249\n",
      "Iteration 67, loss = 1.75375064\n",
      "Iteration 68, loss = 1.75186497\n",
      "Iteration 69, loss = 1.75058487\n",
      "Iteration 70, loss = 1.74921687\n",
      "Iteration 71, loss = 1.74785403\n",
      "Iteration 72, loss = 1.74651590\n",
      "Iteration 73, loss = 1.74584728\n",
      "Iteration 74, loss = 1.74436053\n",
      "Iteration 75, loss = 1.74376851\n",
      "Iteration 76, loss = 1.74279852\n",
      "Iteration 77, loss = 1.74148794\n",
      "Iteration 78, loss = 1.74049511\n",
      "Iteration 79, loss = 1.73958235\n",
      "Iteration 80, loss = 1.73851460\n",
      "Iteration 81, loss = 1.73757586\n",
      "Iteration 82, loss = 1.73681088\n",
      "Iteration 83, loss = 1.73603855\n",
      "Iteration 84, loss = 1.73520058\n",
      "Iteration 85, loss = 1.73456854\n",
      "Iteration 86, loss = 1.73360509\n",
      "Iteration 87, loss = 1.73323438\n",
      "Iteration 88, loss = 1.73236215\n",
      "Iteration 89, loss = 1.73179255\n",
      "Iteration 90, loss = 1.73123995\n",
      "Iteration 91, loss = 1.73053086\n",
      "Iteration 92, loss = 1.72950526\n",
      "Iteration 93, loss = 1.72904003\n",
      "Iteration 94, loss = 1.72853835\n",
      "Iteration 95, loss = 1.72769441\n",
      "Iteration 96, loss = 1.72737785\n",
      "Iteration 97, loss = 1.72707123\n",
      "Iteration 98, loss = 1.72646076\n",
      "Iteration 99, loss = 1.72610889\n",
      "Iteration 100, loss = 1.72548068\n",
      "Iteration 101, loss = 1.72483058\n",
      "Iteration 102, loss = 1.72464736\n",
      "Iteration 103, loss = 1.72429783\n",
      "Iteration 104, loss = 1.72324870\n",
      "Iteration 105, loss = 1.72314612\n",
      "Iteration 106, loss = 1.72233601\n",
      "Iteration 107, loss = 1.72178515\n",
      "Iteration 108, loss = 1.72136502\n",
      "Iteration 109, loss = 1.72103645\n",
      "Iteration 110, loss = 1.72092918\n",
      "Iteration 111, loss = 1.71994373\n",
      "Iteration 112, loss = 1.71986242\n",
      "Iteration 113, loss = 1.71922980\n",
      "Iteration 114, loss = 1.71858246\n",
      "Iteration 115, loss = 1.71853879\n",
      "Iteration 116, loss = 1.71822780\n",
      "Iteration 117, loss = 1.71720887\n",
      "Iteration 118, loss = 1.71723881\n",
      "Iteration 119, loss = 1.71657294\n",
      "Iteration 120, loss = 1.71683708\n",
      "Iteration 121, loss = 1.71580719\n",
      "Iteration 122, loss = 1.71576137\n",
      "Iteration 123, loss = 1.71523481\n",
      "Iteration 124, loss = 1.71477629\n",
      "Iteration 125, loss = 1.71455173\n",
      "Iteration 126, loss = 1.71421216\n",
      "Iteration 127, loss = 1.71421202\n",
      "Iteration 128, loss = 1.71377411\n",
      "Iteration 129, loss = 1.71335722\n",
      "Iteration 130, loss = 1.71296202\n",
      "Iteration 131, loss = 1.71277061\n",
      "Iteration 132, loss = 1.71233702\n",
      "Iteration 133, loss = 1.71181029\n",
      "Iteration 134, loss = 1.71174317\n",
      "Iteration 135, loss = 1.71111519\n",
      "Iteration 136, loss = 1.71112432\n",
      "Iteration 137, loss = 1.71097822\n",
      "Iteration 138, loss = 1.71043209\n",
      "Iteration 139, loss = 1.71035424\n",
      "Iteration 140, loss = 1.70998884\n",
      "Iteration 141, loss = 1.70955889\n",
      "Iteration 142, loss = 1.70925236\n",
      "Iteration 143, loss = 1.70940986\n",
      "Iteration 144, loss = 1.70879621\n",
      "Iteration 145, loss = 1.70923293\n",
      "Iteration 146, loss = 1.70823767\n",
      "Iteration 147, loss = 1.70797107\n",
      "Iteration 148, loss = 1.70801398\n",
      "Iteration 149, loss = 1.70764652\n",
      "Iteration 150, loss = 1.70712400\n",
      "Iteration 151, loss = 1.70703745\n",
      "Iteration 152, loss = 1.70727014\n",
      "Iteration 153, loss = 1.70616148\n",
      "Iteration 154, loss = 1.70651502\n",
      "Iteration 155, loss = 1.70570128\n",
      "Iteration 156, loss = 1.70545070\n",
      "Iteration 157, loss = 1.70516100\n",
      "Iteration 158, loss = 1.70551101\n",
      "Iteration 159, loss = 1.70553148\n",
      "Iteration 160, loss = 1.70421461\n",
      "Iteration 161, loss = 1.70431756\n",
      "Iteration 162, loss = 1.70444824\n",
      "Iteration 163, loss = 1.70343266\n",
      "Iteration 164, loss = 1.70356437\n",
      "Iteration 165, loss = 1.70387952\n",
      "Iteration 166, loss = 1.70303811\n",
      "Iteration 167, loss = 1.70273971\n",
      "Iteration 168, loss = 1.70238070\n",
      "Iteration 169, loss = 1.70256167\n",
      "Iteration 170, loss = 1.70254667\n",
      "Iteration 171, loss = 1.70219864\n",
      "Iteration 172, loss = 1.70151303\n",
      "Iteration 173, loss = 1.70168569\n",
      "Iteration 174, loss = 1.70097269\n",
      "Iteration 175, loss = 1.70082182\n",
      "Iteration 176, loss = 1.70091265\n",
      "Iteration 177, loss = 1.70078066\n",
      "Iteration 178, loss = 1.70057808\n",
      "Iteration 179, loss = 1.70050462\n",
      "Iteration 180, loss = 1.69962273\n",
      "Iteration 181, loss = 1.69997769\n",
      "Iteration 182, loss = 1.69943234\n",
      "Iteration 183, loss = 1.69938656\n",
      "Iteration 184, loss = 1.69929367\n",
      "Iteration 185, loss = 1.69889612\n",
      "Iteration 186, loss = 1.69886136\n",
      "Iteration 187, loss = 1.69841497\n",
      "Iteration 188, loss = 1.69833456\n",
      "Iteration 189, loss = 1.69798566\n",
      "Iteration 190, loss = 1.69780227\n",
      "Iteration 191, loss = 1.69726588\n",
      "Iteration 192, loss = 1.69725206\n",
      "Iteration 193, loss = 1.69735954\n",
      "Iteration 194, loss = 1.69716174\n",
      "Iteration 195, loss = 1.69727508\n",
      "Iteration 196, loss = 1.69693611\n",
      "Iteration 197, loss = 1.69659340\n",
      "Iteration 198, loss = 1.69626442\n",
      "Iteration 199, loss = 1.69574739\n",
      "Iteration 200, loss = 1.69567721\n",
      "Iteration 201, loss = 1.69576299\n",
      "Iteration 202, loss = 1.69520896\n",
      "Iteration 203, loss = 1.69538569\n",
      "Iteration 204, loss = 1.69486295\n",
      "Iteration 205, loss = 1.69495757\n",
      "Iteration 206, loss = 1.69447063\n",
      "Iteration 207, loss = 1.69455255\n",
      "Iteration 208, loss = 1.69404590\n",
      "Iteration 209, loss = 1.69440207\n",
      "Iteration 210, loss = 1.69388234\n",
      "Iteration 211, loss = 1.69409356\n",
      "Iteration 212, loss = 1.69340619\n",
      "Iteration 213, loss = 1.69361974\n",
      "Iteration 214, loss = 1.69391258\n",
      "Iteration 215, loss = 1.69303706\n",
      "Iteration 216, loss = 1.69332877\n",
      "Iteration 217, loss = 1.69262433\n",
      "Iteration 218, loss = 1.69266364\n",
      "Iteration 219, loss = 1.69248863\n",
      "Iteration 220, loss = 1.69246979\n",
      "Iteration 221, loss = 1.69248273\n",
      "Iteration 222, loss = 1.69245032\n",
      "Iteration 223, loss = 1.69190136\n",
      "Iteration 224, loss = 1.69195153\n",
      "Iteration 225, loss = 1.69149572\n",
      "Iteration 226, loss = 1.69109314\n",
      "Iteration 227, loss = 1.69119344\n",
      "Iteration 228, loss = 1.69128206\n",
      "Iteration 229, loss = 1.69134658\n",
      "Iteration 230, loss = 1.69065662\n",
      "Iteration 231, loss = 1.69045803\n",
      "Iteration 232, loss = 1.69056321\n",
      "Iteration 233, loss = 1.69025468\n",
      "Iteration 234, loss = 1.68994793\n",
      "Iteration 235, loss = 1.68991299\n",
      "Iteration 236, loss = 1.68983775\n",
      "Iteration 237, loss = 1.68958709\n",
      "Iteration 238, loss = 1.68971514\n",
      "Iteration 239, loss = 1.68908480\n",
      "Iteration 240, loss = 1.68887305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 241, loss = 1.68915065\n",
      "Iteration 242, loss = 1.68895498\n",
      "Iteration 243, loss = 1.68892112\n",
      "Iteration 244, loss = 1.68834204\n",
      "Iteration 245, loss = 1.68852092\n",
      "Iteration 246, loss = 1.68858452\n",
      "Iteration 247, loss = 1.68808615\n",
      "Iteration 248, loss = 1.68821782\n",
      "Iteration 249, loss = 1.68770308\n",
      "Iteration 250, loss = 1.68755965\n",
      "Iteration 251, loss = 1.68777275\n",
      "Iteration 252, loss = 1.68743684\n",
      "Iteration 253, loss = 1.68706692\n",
      "Iteration 254, loss = 1.68675842\n",
      "Iteration 255, loss = 1.68706131\n",
      "Iteration 256, loss = 1.68632196\n",
      "Iteration 257, loss = 1.68680604\n",
      "Iteration 258, loss = 1.68645123\n",
      "Iteration 259, loss = 1.68706648\n",
      "Iteration 260, loss = 1.68600919\n",
      "Iteration 261, loss = 1.68592275\n",
      "Iteration 262, loss = 1.68622297\n",
      "Iteration 263, loss = 1.68535678\n",
      "Iteration 264, loss = 1.68613808\n",
      "Iteration 265, loss = 1.68525969\n",
      "Iteration 266, loss = 1.68564968\n",
      "Iteration 267, loss = 1.68519498\n",
      "Iteration 268, loss = 1.68516992\n",
      "Iteration 269, loss = 1.68476969\n",
      "Iteration 270, loss = 1.68516821\n",
      "Iteration 271, loss = 1.68440386\n",
      "Iteration 272, loss = 1.68474110\n",
      "Iteration 273, loss = 1.68432288\n",
      "Iteration 274, loss = 1.68457411\n",
      "Iteration 275, loss = 1.68417571\n",
      "Iteration 276, loss = 1.68408983\n",
      "Iteration 277, loss = 1.68357078\n",
      "Iteration 278, loss = 1.68404074\n",
      "Iteration 279, loss = 1.68366008\n",
      "Iteration 280, loss = 1.68327583\n",
      "Iteration 281, loss = 1.68405629\n",
      "Iteration 282, loss = 1.68303784\n",
      "Iteration 283, loss = 1.68372344\n",
      "Iteration 284, loss = 1.68300528\n",
      "Iteration 285, loss = 1.68316766\n",
      "Iteration 286, loss = 1.68264691\n",
      "Iteration 287, loss = 1.68306542\n",
      "Iteration 288, loss = 1.68267464\n",
      "Iteration 289, loss = 1.68204115\n",
      "Iteration 290, loss = 1.68275124\n",
      "Iteration 291, loss = 1.68223590\n",
      "Iteration 292, loss = 1.68212252\n",
      "Iteration 293, loss = 1.68196856\n",
      "Iteration 294, loss = 1.68214395\n",
      "Iteration 295, loss = 1.68201706\n",
      "Iteration 296, loss = 1.68163756\n",
      "Iteration 297, loss = 1.68115932\n",
      "Iteration 298, loss = 1.68155863\n",
      "Iteration 299, loss = 1.68158053\n",
      "Iteration 300, loss = 1.68134115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "accuracy = 0.46\n",
      "accuracy of train data=0.46\n"
     ]
    }
   ],
   "source": [
    "# All classifiers\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "names = [\"Nearest Neighbors\",  \"Decision Tree\",\n",
    "         \"Random Forest\", \"AdaBoost\", \"Naive Bayes\", \"LDA\", \"QDA\",\"MLP\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(n_neighbors=3),\n",
    "    DecisionTreeClassifier(max_depth=24),\n",
    "    RandomForestClassifier(max_depth=25, n_estimators=10, max_features=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    LDA(),\n",
    "    QDA(),\n",
    "    MLPClassifier(hidden_layer_sizes=(25,20),alpha=0.002,verbose=True ,  max_iter=300)]\n",
    "\n",
    "#X = StandardScaler().fit_transform(emg_17class['data'])\n",
    "\n",
    "x_train , x_test,y_train,y_test=train_test_split(emg_17class['data'], emg_17class['label'],test_size=0.25,random_state=42,stratify=emg_17class['label']) \n",
    "\n",
    "\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "# iterate over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred=clf.predict(x_test)\n",
    "    print(name)\n",
    "    print(\"accuracy = %.2f\"%accuracy_score(y_test,y_pred))\n",
    "    y_pred_train=clf.predict(x_train)\n",
    "    accuracy=np.mean(y_pred_train==y_train)\n",
    "    print(\"accuracy of train data=%.2f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA()\n",
    "S_ = ica.fit_transform(dataemg['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((535783, 10), (535783,), (178595, 10), (178595,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train , x_test,y_train,y_test=train_test_split(S_ , dataemg['label'],test_size=0.25,random_state=42,stratify=dataemg['label']) \n",
    "np.shape(x_train),np.shape(y_train),np.shape(x_test),np.shape(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.88537988\n",
      "Iteration 2, loss = 1.59769428\n",
      "Iteration 3, loss = 1.53031436\n",
      "Iteration 4, loss = 1.52215710\n",
      "Iteration 5, loss = 1.51898208\n",
      "Iteration 6, loss = 1.51698690\n",
      "Iteration 7, loss = 1.51524429\n",
      "Iteration 8, loss = 1.51302451\n",
      "Iteration 9, loss = 1.51087199\n",
      "Iteration 10, loss = 1.50636120\n",
      "Iteration 11, loss = 1.49706078\n",
      "Iteration 12, loss = 1.48531964\n",
      "Iteration 13, loss = 1.47603550\n",
      "Iteration 14, loss = 1.47015493\n",
      "Iteration 15, loss = 1.46485999\n",
      "Iteration 16, loss = 1.45966450\n",
      "Iteration 17, loss = 1.45268255\n",
      "Iteration 18, loss = 1.44690548\n",
      "Iteration 19, loss = 1.44205838\n",
      "Iteration 20, loss = 1.43791941\n",
      "Iteration 21, loss = 1.43477274\n",
      "Iteration 22, loss = 1.43182159\n",
      "Iteration 23, loss = 1.42933936\n",
      "Iteration 24, loss = 1.42758301\n",
      "Iteration 25, loss = 1.42591121\n",
      "Iteration 26, loss = 1.42452316\n",
      "Iteration 27, loss = 1.42282137\n",
      "Iteration 28, loss = 1.42101801\n",
      "Iteration 29, loss = 1.41980008\n",
      "Iteration 30, loss = 1.41828552\n",
      "Iteration 31, loss = 1.41707436\n",
      "Iteration 32, loss = 1.41600402\n",
      "Iteration 33, loss = 1.41465138\n",
      "Iteration 34, loss = 1.41363455\n",
      "Iteration 35, loss = 1.41246675\n",
      "Iteration 36, loss = 1.41115240\n",
      "Iteration 37, loss = 1.40983906\n",
      "Iteration 38, loss = 1.40871702\n",
      "Iteration 39, loss = 1.40746472\n",
      "Iteration 40, loss = 1.40600894\n",
      "Iteration 41, loss = 1.40401376\n",
      "Iteration 42, loss = 1.40259806\n",
      "Iteration 43, loss = 1.40032320\n",
      "Iteration 44, loss = 1.39755797\n",
      "Iteration 45, loss = 1.39538638\n",
      "Iteration 46, loss = 1.39251216\n",
      "Iteration 47, loss = 1.38958495\n",
      "Iteration 48, loss = 1.38714585\n",
      "Iteration 49, loss = 1.38438908\n",
      "Iteration 50, loss = 1.38157122\n",
      "Iteration 51, loss = 1.37883435\n",
      "Iteration 52, loss = 1.37622461\n",
      "Iteration 53, loss = 1.37368454\n",
      "Iteration 54, loss = 1.37110861\n",
      "Iteration 55, loss = 1.36913707\n",
      "Iteration 56, loss = 1.36704517\n",
      "Iteration 57, loss = 1.36492229\n",
      "Iteration 58, loss = 1.36391578\n",
      "Iteration 59, loss = 1.36233843\n",
      "Iteration 60, loss = 1.36130198\n",
      "Iteration 61, loss = 1.35987584\n",
      "Iteration 62, loss = 1.35899990\n",
      "Iteration 63, loss = 1.35754647\n",
      "Iteration 64, loss = 1.35673768\n",
      "Iteration 65, loss = 1.35597668\n",
      "Iteration 66, loss = 1.35481357\n",
      "Iteration 67, loss = 1.35401316\n",
      "Iteration 68, loss = 1.35296369\n",
      "Iteration 69, loss = 1.35220348\n",
      "Iteration 70, loss = 1.35109935\n",
      "Iteration 71, loss = 1.35000607\n",
      "Iteration 72, loss = 1.34851969\n",
      "Iteration 73, loss = 1.34814777\n",
      "Iteration 74, loss = 1.34661399\n",
      "Iteration 75, loss = 1.34582120\n",
      "Iteration 76, loss = 1.34472499\n",
      "Iteration 77, loss = 1.34354332\n",
      "Iteration 78, loss = 1.34256897\n",
      "Iteration 79, loss = 1.34120239\n",
      "Iteration 80, loss = 1.34009817\n",
      "Iteration 81, loss = 1.33878065\n",
      "Iteration 82, loss = 1.33798593\n",
      "Iteration 83, loss = 1.33681647\n",
      "Iteration 84, loss = 1.33548124\n",
      "Iteration 85, loss = 1.33448536\n",
      "Iteration 86, loss = 1.33341770\n",
      "Iteration 87, loss = 1.33243586\n",
      "Iteration 88, loss = 1.33176916\n",
      "Iteration 89, loss = 1.33018799\n",
      "Iteration 90, loss = 1.32960961\n",
      "Iteration 91, loss = 1.32829767\n",
      "Iteration 92, loss = 1.32678355\n",
      "Iteration 93, loss = 1.32605110\n",
      "Iteration 94, loss = 1.32492955\n",
      "Iteration 95, loss = 1.32369404\n",
      "Iteration 96, loss = 1.32255394\n",
      "Iteration 97, loss = 1.32167153\n",
      "Iteration 98, loss = 1.32110696\n",
      "Iteration 99, loss = 1.31988863\n",
      "Iteration 100, loss = 1.31908273\n",
      "Iteration 101, loss = 1.31858017\n",
      "Iteration 102, loss = 1.31769749\n",
      "Iteration 103, loss = 1.31671399\n",
      "Iteration 104, loss = 1.31634053\n",
      "Iteration 105, loss = 1.31550547\n",
      "Iteration 106, loss = 1.31449352\n",
      "Iteration 107, loss = 1.31422099\n",
      "Iteration 108, loss = 1.31376699\n",
      "Iteration 109, loss = 1.31338504\n",
      "Iteration 110, loss = 1.31250171\n",
      "Iteration 111, loss = 1.31169751\n",
      "Iteration 112, loss = 1.31189429\n",
      "Iteration 113, loss = 1.31071854\n",
      "Iteration 114, loss = 1.31081552\n",
      "Iteration 115, loss = 1.31041806\n",
      "Iteration 116, loss = 1.30974212\n",
      "Iteration 117, loss = 1.30962470\n",
      "Iteration 118, loss = 1.30898122\n",
      "Iteration 119, loss = 1.30891789\n",
      "Iteration 120, loss = 1.30830423\n",
      "Iteration 121, loss = 1.30779581\n",
      "Iteration 122, loss = 1.30701292\n",
      "Iteration 123, loss = 1.30689536\n",
      "Iteration 124, loss = 1.30685682\n",
      "Iteration 125, loss = 1.30608062\n",
      "Iteration 126, loss = 1.30590559\n",
      "Iteration 127, loss = 1.30552339\n",
      "Iteration 128, loss = 1.30548260\n",
      "Iteration 129, loss = 1.30493248\n",
      "Iteration 130, loss = 1.30444941\n",
      "Iteration 131, loss = 1.30433169\n",
      "Iteration 132, loss = 1.30385611\n",
      "Iteration 133, loss = 1.30362384\n",
      "Iteration 134, loss = 1.30309868\n",
      "Iteration 135, loss = 1.30318180\n",
      "Iteration 136, loss = 1.30227987\n",
      "Iteration 137, loss = 1.30212710\n",
      "Iteration 138, loss = 1.30165035\n",
      "Iteration 139, loss = 1.30112316\n",
      "Iteration 140, loss = 1.30126854\n",
      "Iteration 141, loss = 1.30080887\n",
      "Iteration 142, loss = 1.29975656\n",
      "Iteration 143, loss = 1.30009424\n",
      "Iteration 144, loss = 1.29866191\n",
      "Iteration 145, loss = 1.29840157\n",
      "Iteration 146, loss = 1.29811078\n",
      "Iteration 147, loss = 1.29753449\n",
      "Iteration 148, loss = 1.29700806\n",
      "Iteration 149, loss = 1.29664938\n",
      "Iteration 150, loss = 1.29563834\n",
      "Iteration 151, loss = 1.29494480\n",
      "Iteration 152, loss = 1.29448424\n",
      "Iteration 153, loss = 1.29360909\n",
      "Iteration 154, loss = 1.29344968\n",
      "Iteration 155, loss = 1.29270519\n",
      "Iteration 156, loss = 1.29263750\n",
      "Iteration 157, loss = 1.29210413\n",
      "Iteration 158, loss = 1.29187596\n",
      "Iteration 159, loss = 1.29120289\n",
      "Iteration 160, loss = 1.29145730\n",
      "Iteration 161, loss = 1.29040078\n",
      "Iteration 162, loss = 1.29040790\n",
      "Iteration 163, loss = 1.28997842\n",
      "Iteration 164, loss = 1.28959055\n",
      "Iteration 165, loss = 1.28916963\n",
      "Iteration 166, loss = 1.28884062\n",
      "Iteration 167, loss = 1.28903390\n",
      "Iteration 168, loss = 1.28794980\n",
      "Iteration 169, loss = 1.28787472\n",
      "Iteration 170, loss = 1.28788668\n",
      "Iteration 171, loss = 1.28722869\n",
      "Iteration 172, loss = 1.28717052\n",
      "Iteration 173, loss = 1.28651412\n",
      "Iteration 174, loss = 1.28620670\n",
      "Iteration 175, loss = 1.28587198\n",
      "Iteration 176, loss = 1.28636463\n",
      "Iteration 177, loss = 1.28543112\n",
      "Iteration 178, loss = 1.28514191\n",
      "Iteration 179, loss = 1.28478398\n",
      "Iteration 180, loss = 1.28508583\n",
      "Iteration 181, loss = 1.28475792\n",
      "Iteration 182, loss = 1.28410557\n",
      "Iteration 183, loss = 1.28356518\n",
      "Iteration 184, loss = 1.28336785\n",
      "Iteration 185, loss = 1.28336942\n",
      "Iteration 186, loss = 1.28327826\n",
      "Iteration 187, loss = 1.28306560\n",
      "Iteration 188, loss = 1.28282611\n",
      "Iteration 189, loss = 1.28226061\n",
      "Iteration 190, loss = 1.28183344\n",
      "Iteration 191, loss = 1.28181471\n",
      "Iteration 192, loss = 1.28202193\n",
      "Iteration 193, loss = 1.28132940\n",
      "Iteration 194, loss = 1.28133808\n",
      "Iteration 195, loss = 1.28125406\n",
      "Iteration 196, loss = 1.28090229\n",
      "Iteration 197, loss = 1.28059210\n",
      "Iteration 198, loss = 1.28058491\n",
      "Iteration 199, loss = 1.27994590\n",
      "Iteration 200, loss = 1.28004643\n",
      "Iteration 201, loss = 1.27943395\n",
      "Iteration 202, loss = 1.27931886\n",
      "Iteration 203, loss = 1.27861454\n",
      "Iteration 204, loss = 1.27862803\n",
      "Iteration 205, loss = 1.27865978\n",
      "Iteration 206, loss = 1.27819992\n",
      "Iteration 207, loss = 1.27789907\n",
      "Iteration 208, loss = 1.27794701\n",
      "Iteration 209, loss = 1.27749076\n",
      "Iteration 210, loss = 1.27757987\n",
      "Iteration 211, loss = 1.27718322\n",
      "Iteration 212, loss = 1.27669371\n",
      "Iteration 213, loss = 1.27670895\n",
      "Iteration 214, loss = 1.27651489\n",
      "Iteration 215, loss = 1.27598548\n",
      "Iteration 216, loss = 1.27622395\n",
      "Iteration 217, loss = 1.27626341\n",
      "Iteration 218, loss = 1.27589012\n",
      "Iteration 219, loss = 1.27570420\n",
      "Iteration 220, loss = 1.27554009\n",
      "Iteration 221, loss = 1.27528743\n",
      "Iteration 222, loss = 1.27503048\n",
      "Iteration 223, loss = 1.27487806\n",
      "Iteration 224, loss = 1.27463290\n",
      "Iteration 225, loss = 1.27454215\n",
      "Iteration 226, loss = 1.27406314\n",
      "Iteration 227, loss = 1.27398385\n",
      "Iteration 228, loss = 1.27435501\n",
      "Iteration 229, loss = 1.27360578\n",
      "Iteration 230, loss = 1.27334470\n",
      "Iteration 231, loss = 1.27336650\n",
      "Iteration 232, loss = 1.27319344\n",
      "Iteration 233, loss = 1.27360902\n",
      "Iteration 234, loss = 1.27269921\n",
      "Iteration 235, loss = 1.27308838\n",
      "Iteration 236, loss = 1.27287683\n",
      "Iteration 237, loss = 1.27255438\n",
      "Iteration 238, loss = 1.27230863\n",
      "Iteration 239, loss = 1.27226378\n",
      "Iteration 240, loss = 1.27246369\n",
      "Iteration 241, loss = 1.27211826\n",
      "Iteration 242, loss = 1.27198738\n",
      "Iteration 243, loss = 1.27185018\n",
      "Iteration 244, loss = 1.27159881\n",
      "Iteration 245, loss = 1.27183801\n",
      "Iteration 246, loss = 1.27145595\n",
      "Iteration 247, loss = 1.27100323\n",
      "Iteration 248, loss = 1.27086075\n",
      "Iteration 249, loss = 1.27107681\n",
      "Iteration 250, loss = 1.27066680\n",
      "Iteration 251, loss = 1.27077787\n",
      "Iteration 252, loss = 1.27090713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 1.27057843\n",
      "Iteration 254, loss = 1.27057597\n",
      "Iteration 255, loss = 1.27020021\n",
      "Iteration 256, loss = 1.27018010\n",
      "Iteration 257, loss = 1.26998429\n",
      "Iteration 258, loss = 1.27041464\n",
      "Iteration 259, loss = 1.26982684\n",
      "Iteration 260, loss = 1.26986951\n",
      "Iteration 261, loss = 1.26975242\n",
      "Iteration 262, loss = 1.26950187\n",
      "Iteration 263, loss = 1.26946803\n",
      "Iteration 264, loss = 1.26958921\n",
      "Iteration 265, loss = 1.26897130\n",
      "Iteration 266, loss = 1.26900708\n",
      "Iteration 267, loss = 1.26848054\n",
      "Iteration 268, loss = 1.26846341\n",
      "Iteration 269, loss = 1.26871201\n",
      "Iteration 270, loss = 1.26797059\n",
      "Iteration 271, loss = 1.26860680\n",
      "Iteration 272, loss = 1.26782280\n",
      "Iteration 273, loss = 1.26778681\n",
      "Iteration 274, loss = 1.26785048\n",
      "Iteration 275, loss = 1.26792327\n",
      "Iteration 276, loss = 1.26751164\n",
      "Iteration 277, loss = 1.26751359\n",
      "Iteration 278, loss = 1.26705253\n",
      "Iteration 279, loss = 1.26734068\n",
      "Iteration 280, loss = 1.26625638\n",
      "Iteration 281, loss = 1.26697008\n",
      "Iteration 282, loss = 1.26669596\n",
      "Iteration 283, loss = 1.26644493\n",
      "Iteration 284, loss = 1.26597693\n",
      "Iteration 285, loss = 1.26595394\n",
      "Iteration 286, loss = 1.26608040\n",
      "Iteration 287, loss = 1.26546837\n",
      "Iteration 288, loss = 1.26624317\n",
      "Iteration 289, loss = 1.26530418\n",
      "Iteration 290, loss = 1.26575833\n",
      "Iteration 291, loss = 1.26586661\n",
      "Iteration 292, loss = 1.26570425\n",
      "Iteration 293, loss = 1.26565834\n",
      "Iteration 294, loss = 1.26497489\n",
      "Iteration 295, loss = 1.26456563\n",
      "Iteration 296, loss = 1.26503571\n",
      "Iteration 297, loss = 1.26439862\n",
      "Iteration 298, loss = 1.26445309\n",
      "Iteration 299, loss = 1.26438966\n",
      "Iteration 300, loss = 1.26453229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of test data=0.66\n",
      "accuracy of train data=0.67\n"
     ]
    }
   ],
   "source": [
    "#MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf=MLPClassifier(hidden_layer_sizes=(25,20),alpha=0.002,verbose=True ,  max_iter=300)\n",
    "#clf=MLPClassifier(hidden_layer_sizes=(100),alpha=0.002,verbose=True ,  max_iter=300)\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred=clf.predict(x_test)\n",
    "accuracy=np.mean(y_pred==y_test)\n",
    "print(\"accuracy of test data=%.2f\" %accuracy)\n",
    "y_pred_train=clf.predict(x_train)\n",
    "accuracy=np.mean(y_pred_train==y_train)\n",
    "print(\"accuracy of train data=%.2f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of test data=0.62\n",
      "accuracy of train data=0.62\n"
     ]
    }
   ],
   "source": [
    "#LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "clf=LDA()\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred=clf.predict(x_test)\n",
    "accuracy=np.mean(y_pred==y_test)\n",
    "print(\"accuracy of test data=%.2f\" %accuracy)\n",
    "y_pred_train=clf.predict(x_train)\n",
    "accuracy=np.mean(y_pred_train==y_train)\n",
    "print(\"accuracy of train data=%.2f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of test data=0.63\n",
      "accuracy of train data=0.63\n"
     ]
    }
   ],
   "source": [
    "#QDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "clf=QDA()\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred=clf.predict(x_test)\n",
    "accuracy=np.mean(y_pred==y_test)\n",
    "print(\"accuracy of test data=%.2f\" %accuracy)\n",
    "y_pred_train=clf.predict(x_train)\n",
    "accuracy=np.mean(y_pred_train==y_train)\n",
    "print(\"accuracy of train data=%.2f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of test data=0.86\n",
      "accuracy of train data=0.99\n"
     ]
    }
   ],
   "source": [
    "#RF\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "clf = RF(max_depth=200, random_state=0)\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred=clf.predict(x_test)\n",
    "accuracy=np.mean(y_pred==y_test)\n",
    "print(\"accuracy of test data=%.2f\" %accuracy)\n",
    "y_pred_train=clf.predict(x_train)\n",
    "accuracy=np.mean(y_pred_train==y_train)\n",
    "print(\"accuracy of train data=%.2f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of test data=0.80\n",
      "accuracy of train data=0.98\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#clf=DecisionTreeClassifier(max_depth=200) #overfit\n",
    "clf=DecisionTreeClassifier(max_depth=34)\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred=clf.predict(x_test)\n",
    "accuracy=np.mean(y_pred==y_test)\n",
    "print(\"accuracy of test data=%.2f\" %accuracy)\n",
    "y_pred_train=clf.predict(x_train)\n",
    "accuracy=np.mean(y_pred_train==y_train)\n",
    "print(\"accuracy of train data=%.2f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of test data=0.87\n",
      "accuracy of train data=0.93\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred=clf.predict(x_test)\n",
    "accuracy=np.mean(y_pred==y_test)\n",
    "print(\"accuracy of test data=%.2f\" %accuracy)\n",
    "y_pred_train=clf.predict(x_train)\n",
    "accuracy=np.mean(y_pred_train==y_train)\n",
    "print(\"accuracy of train data=%.2f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
